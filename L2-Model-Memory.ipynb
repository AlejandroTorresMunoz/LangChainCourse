{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7ef713",
   "metadata": {},
   "source": [
    "In this notebook we are going explore how the different ways to add memory to our LLM model.\n",
    "\n",
    "As first step, we create the chat model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c1ee44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "chat_model = ChatOllama(model=\"qwen2.5:3b\", temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968d510",
   "metadata": {},
   "source": [
    "The objects that we are going to explore are the these:\n",
    "\n",
    "-   **ConversationBufferMemory**: Keeps the entire conversation history in memory.\n",
    "-   **ConversationBufferWindowMemory**: Keeps only the last few exchanges (a sliding window of recent messages).\n",
    "-   **ConversationTokenBufferMemory**: Keeps messages up to a token limit,  emptying the whole memory when the limit is reached.\n",
    "-   **ConversationSummaryMemory**: Keeps a summary of previous dialogue, not the full text, to preserve key context efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f134cabd",
   "metadata": {},
   "source": [
    "The first object we use to set the memory of the model is the class `ConversationBufferMemory` class. Using this object, we add a memory buffer to out chat pipeline, able to converse the knowldge of previous messages.\n",
    "\n",
    "In additino, we are going to use the class `ConversationChain` class as auxiliar object to create a chat with memory context. It simplifies the use of chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a85a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Alejandro.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Hello Alejandro! It's nice to meet you. How can I assist you today? Are there any specific topics or questions you'd like to discuss? Whether it's about technology, general knowledge, or just a casual chat, feel free to share what interests you.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Alejandro.\n",
      "AI: Hello Alejandro! It's nice to meet you. How can I assist you today? Are there any specific topics or questions you'd like to discuss? Whether it's about technology, general knowledge, or just a casual chat, feel free to share what interests you.\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Your name is Alejandro.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(llm=chat_model, memory=memory, verbose=True)\n",
    "response = conversation.predict(input=\"Hi, my name is Alejandro.\")\n",
    "print(response)\n",
    "response = conversation.predict(input=\"What is my name?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198d5b51",
   "metadata": {},
   "source": [
    "We can access to the memory buffer, and observe the stored messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f62c0485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Alejandro.\n",
      "AI: Hello Alejandro! It's nice to meet you. How can I assist you today? Are there any specific topics or questions you'd like to discuss? Whether it's about technology, general knowledge, or just a casual chat, feel free to share what interests you.\n",
      "Human: What is my name?\n",
      "AI: Your name is Alejandro.\n",
      "{'history': \"Human: Hi, my name is Alejandro.\\nAI: Hello Alejandro! It's nice to meet you. How can I assist you today? Are there any specific topics or questions you'd like to discuss? Whether it's about technology, general knowledge, or just a casual chat, feel free to share what interests you.\\nHuman: What is my name?\\nAI: Your name is Alejandro.\"}\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4034cd1",
   "metadata": {},
   "source": [
    "We can also add memory context to the memory buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c0fe639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': \"Human: Hi\\nAI: What's up\"}\n",
      "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n",
    "print(memory.load_memory_variables({}))\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32078a6d",
   "metadata": {},
   "source": [
    "Another possibility to create a memory is to use the object `ConversationBufferMemory`. We have to set the number of exchanges to keep in memory through the input variable `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41fcf1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Alejandro.\n",
      "AI: Hello Alejandro! How can I assist you today?\n",
      "Human: My surname is Torres\n",
      "AI: Hi Mr.Torres!\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I apologize, but there seems to be a mix-up in our conversation. You mentioned your surname as \"Torres,\" and now you're asking what your name is. Could you please clarify? Are you looking for information about your given name or are you unsure of your full name?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Alejandro.\n",
      "AI: Hello Alejandro! How can I assist you today?\n",
      "Human: My surname is Torres\n",
      "AI: Hi Mr.Torres!\n",
      "Human: What is my name?\n",
      "AI: I apologize, but there seems to be a mix-up in our conversation. You mentioned your surname as \"Torres,\" and now you're asking what your name is. Could you please clarify? Are you looking for information about your given name or are you unsure of your full name?\n",
      "Human: What is my surname?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I see, it appears there might be some confusion in our initial exchange. You mentioned that your surname is \"Torres.\" Is this correct? If so, then your surname is indeed Torres. Are you looking for more information about surnames or perhaps assistance with something else related to names and identities?\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory(k=1) # Keep only the last exchange\n",
    "memory.save_context({\"input\": \"Hi, my name is Alejandro.\"}, {\"output\": \"Hello Alejandro! How can I assist you today?\"})\n",
    "memory.save_context({\"input\": \"My surname is Torres\"}, {\"output\": \"Hi Mr.Torres!\"})\n",
    "conversation = ConversationChain(llm=chat_model, memory=memory, verbose=True)\n",
    "response = conversation.predict(input=\"What is my name?\")\n",
    "print(response)\n",
    "response = conversation.predict(input=\"What is my surname?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657f4f3",
   "metadata": {},
   "source": [
    "We can observe that the model has forgotten the first exchange, so it will keep the last `k` exchanges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e0bee",
   "metadata": {},
   "source": [
    "The next option to be explored is the `ConversationTokenBufferMemory`. We define the limit of tokens to be saved in memory through the variable `max_token_limit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e772d23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "     --------------------------------------- 12.0/12.0 MB 18.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "     ------------------------------------- 564.3/564.3 kB 11.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 24.4 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "     ------------------------------------- 320.2/320.2 kB 19.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aleja\\desktop\\dev\\langchaincourse\\venv\\lib\\site-packages (from requests->transformers) (2025.7.9)\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.35.3 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388473ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of memory input tokens: 11\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "AI: eggs, milk, bread, butter. \n",
      "Human: Could you tell me the shopping list?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Certainly! Based on the items you've mentioned—eggs, milk, bread, and butter—I can provide a basic shopping list for your needs:\n",
      "\n",
      "1. Eggs\n",
      "2. Milk (you might want to specify whether it's whole milk, skim milk, or another type)\n",
      "3. Bread (considering you may need multiple types like white, whole wheat, or gluten-free)\n",
      "4. Butter\n",
      "\n",
      "Please note that this is a general list and the quantities can vary depending on your household size and dietary preferences. If you have any specific items in mind or if there are other things you usually buy at the grocery store, feel free to let me know so I can add them to your shopping list!\n",
      "{'history': ''}\n",
      "Number of memory input tokens: 11\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Could you tell me the shopping list?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I'm sorry for any confusion, but I don't have access to your current shopping list as my knowledge is based on data available up until October 2021. Since then, no specific information about your shopping list has been stored in my context. If you could provide me with the details of what items are already in your possession or need to be purchased, I can certainly help you create a new shopping list tailored to your needs.\n",
      "{'history': ''}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "input_prompt = \"eggs, milk, bread, butter. \"\n",
    "chat_model = ChatOllama(model=\"qwen2.5:3b\", temperature=0.0)\n",
    "num_input_tokens = chat_model.get_num_tokens(input_prompt) + 1 # Add 1 for the END token\n",
    "print(f\"Number of memory input tokens: {num_input_tokens}\")\n",
    "memory = ConversationTokenBufferMemory(llm=chat_model, max_token_limit=num_input_tokens+1) # Keep messages up to a token limit\n",
    "memory.save_context({\"input\": \"Please, memorize the following shopping list\"}, {\"output\": input_prompt})\n",
    "conversation = ConversationChain(llm=chat_model, memory=memory, verbose=True)\n",
    "response = conversation.predict(input=\"Could you tell me the shopping list?\")\n",
    "print(response)\n",
    "print(memory.load_memory_variables({}))\n",
    "extended_prompt = input_prompt + \" cheese, tomatoes, lettuce, chicken, rice, pasta, olive oil, coffee, tea, sugar, flour, salt, pepper.\"\n",
    "# num_input_tokens = chat_model.get_num_tokens(extended_prompt)\n",
    "print(f\"Number of memory input tokens: {num_input_tokens}\")\n",
    "memory = ConversationTokenBufferMemory(llm=chat_model, max_token_limit=num_input_tokens) # Keep messages up to a token limit\n",
    "memory.save_context({\"input\": \"Please, memorize the following shopping list\"}, {\"output\": extended_prompt})\n",
    "conversation = ConversationChain(llm=chat_model, memory=memory, verbose=True)\n",
    "response = conversation.predict(input=\"Could you tell me the shopping list?\")\n",
    "print(response)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37555c91",
   "metadata": {},
   "source": [
    "As you can observe, it's important to add 1 token to the count to consider the END token. Also, we can see that if the memory buffer is overflowed with the number of tokens, it get's empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ad956",
   "metadata": {},
   "source": [
    "And the last object that we are going to implement is the class `CoversationSummaryBufferMemory`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6c145dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_16000\\2086351974.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=chat_model, max_token_limit=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human initiates a casual greeting and then mentions they are not doing anything specific. The AI responds with a casual acknowledgment and then asks about daily schedules.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "A great demo for showing off the capabilities of a Language Model (LLM) like LangChain could include several engaging and informative segments:\n",
      "\n",
      "1. **Text Generation**: You can demonstrate how well your model performs in generating coherent, contextually appropriate text. This could involve showing it write emails, short stories, product descriptions, or even poetry.\n",
      "\n",
      "2. **Question Answering**: Showcase the model's ability to answer questions based on a given passage of text. For example, you might ask it about specific details from an article or provide it with a complex question and see how accurately it responds.\n",
      "\n",
      "3. **Translation**: If your LLM supports multiple languages, demonstrate its translation capabilities by translating short passages between different languages. This can be particularly useful for showing off the model's accuracy and fluency in various dialects and contexts.\n",
      "\n",
      "4. **Summarization**: Show how well the model can summarize long documents or articles into concise summaries. You could also compare it to human-written summaries to highlight its efficiency and effectiveness.\n",
      "\n",
      "5. **Dialogue Simulation**: Create a dialogue between two characters using your LLM, where one character asks questions and the other responds based on predefined knowledge bases or context. This can be particularly effective for showing off how well the model understands different conversational styles and contexts.\n",
      "\n",
      "6. **Creative Writing Prompting**: Use prompts to generate creative writing pieces such as short stories, scripts, or even entire novels. This not only showcases your LLM's creativity but also its ability to handle more complex narratives.\n",
      "\n",
      "7. **Fact Checking**: Show how well the model can verify facts from a given text. You could provide it with a list of statements and have it check their accuracy against reliable sources.\n",
      "\n",
      "8. **Personalized Recommendations**: If applicable, demonstrate how your LLM can generate personalized recommendations based on user data or preferences. This could be for product suggestions, movie recommendations, or even dietary advice.\n",
      "\n",
      "Each of these demos would give the customer insight into different aspects of LangChain's capabilities and help them understand its potential applications in various industries.\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOllama\n",
    "\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "chat_model = ChatOllama(model=\"qwen2.5:3b\", temperature=0.0)\n",
    "memory = ConversationSummaryBufferMemory(llm=chat_model, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})\n",
    "conversation = ConversationChain(llm=chat_model, memory=memory, verbose=True)\n",
    "response = conversation.predict(input=\"What would be a good demo to show?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
