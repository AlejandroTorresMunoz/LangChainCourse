{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "899fa408",
   "metadata": {},
   "source": [
    "In this notebook we are going to explore the chains inside the LangChain framework. We are going to explore the following elements:\n",
    "\n",
    "TODO: Dame euna explciación en inglés para cada uno de estos elementos. Sé breve.\n",
    "-   **LLMChain**: A single chain that sends input to a language model using a prompt template and returns the model’s output. It is the basic building block for interacting with LLMs.\n",
    "-   **SequentialChains**: Chains that execute multiple sub-chains in sequence, passing the output of one as the input to the next. Useful for building multi-step workflows.\n",
    "    -   **SimpleSequentialChain**: A simplified version of a sequential chain that automatically passes each chain’s output to the next. Best for linear, straightforward pipelines with minimal configuration.\n",
    "    -   **SequentialChain**: A more flexible sequential chain that allows explicit mapping of input and output variables between steps, supporting complex workflows with multiple inputs and outputs.\n",
    "-   **RouterChain**: A chain that dynamically decides which sub-chain to run based on the input. It acts as a “router” to direct tasks to the appropriate specialized chain or model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5a522",
   "metadata": {},
   "source": [
    "# LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "583c847e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A suitable and descriptive name for a company that specializes in making Queen Size Sheet Sets could be \"Queenly Sheets.\" This name not only reflects the product category but also conveys elegance, comfort, and quality. Another option could be \"Royal Rest,\" which combines the idea of luxury with rest and relaxation.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chat_model = ChatOllama(model=\"qwen2.5:3b\", temperature=0.0)\n",
    "prompt = ChatPromptTemplate.from_template(\"What is the best name to describe a company that makes {product}?\")\n",
    "chain = LLMChain(llm=chat_model, prompt=prompt)\n",
    "product = \"Queen Size Sheet Set\"\n",
    "response = chain.run(product)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc28ca3e",
   "metadata": {},
   "source": [
    "# SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82968fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mA creative and catchy name for a company that specializes in making Queen Size Sheet Sets could be \"Queenly Sheets.\" This name not only reflects the product category but also conveys elegance, comfort, and quality. Another option could be \"Royal Rest,\" which combines the idea of royalty with rest and relaxation. Both names aim to capture the essence of luxury associated with queen size bedding sets.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\"Queenly Sheets\" or \"Royal Rest\": catchy names for a company specializing in luxurious Queen Size Sheet Sets, evoking elegance, comfort, and quality.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\"Queenly Sheets\" or \"Royal Rest\": catchy names for a company specializing in luxurious Queen Size Sheet Sets, evoking elegance, comfort, and quality.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = ChatOllama(model=\"qwen2.5:3b\", temperature=0.0)\n",
    "first_prompt = ChatPromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# Chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "overall_simple_chain = SimpleSequentialChain(\n",
    "    chains=[chain_one, chain_two], verbose=True)\n",
    "product = \"Queen Size Sheet Set\"\n",
    "response = overall_simple_chain.run(product)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9158aa",
   "metadata": {},
   "source": [
    "# SequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83143f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_30908\\1378626460.py:52: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = overall_chain(review)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'Review': 'Me encantó este producto! La calidad es excelente y superó mis expectativas. Lo recomiendo a todos.', 'English_Review': 'I loved this product! The quality is excellent and exceeded my expectations. I recommend it to everyone.', 'summary': \"The reviewer highly praises the product's excellent quality that surpassed their expectations and recommends it to others.\", 'followup_message': 'Resumen en español: El revisor elogia la excelente calidad del producto que superó sus expectativas y recomienda su compra a otros.\\n\\nResponse follow-up (in Spanish): La experiencia de este cliente es un testimonio claro de la excelencia del producto. Su recomendación nos anima a considerarlo como una opción segura para aquellos en búsqueda de alta calidad.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain, SequentialChain\n",
    "\n",
    "chat_model = ChatOllama(model=\"qwen2.5:3b\", temperature=0.0)\n",
    "# Prompt template 1: Translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=chat_model, prompt=first_prompt, output_key=\"English_Review\")\n",
    "# Prompt template 2: summarize review\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=chat_model, prompt=second_prompt, output_key=\"summary\")\n",
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# Prompt template 3: detect language\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=chat_model, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n",
    "# Prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=chat_model, prompt=fourth_prompt,output_key=\"followup_message\")\n",
    "\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "review = \"\"\"Me encantó este producto! La calidad es excelente y superó mis expectativas. Lo recomiendo a todos.\"\"\"\n",
    "response = overall_chain(review)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828b712c",
   "metadata": {},
   "source": [
    "We can observe that we got a dict as output, where each key is each output of a single chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd8dca1",
   "metadata": {},
   "source": [
    "# RouterChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef4967b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt for each subject area\n",
    "\n",
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. \n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "# Create a list of prompt infos\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]\n",
    "\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "chat_model = ChatOllama(model=\"qwen2.5:3b\", temperature=0.0)\n",
    "destination_chains = {}\n",
    "# Create a dict of destination chains. Each key is the name of the subject area\n",
    "# and each value is an LLMChain with the corresponding prompt template\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt_template = ChatPromptTemplate.from_template(prompt_template)\n",
    "    chain = LLMChain(\n",
    "        llm=chat_model,\n",
    "        prompt=prompt_template\n",
    "    )\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "# Create a string that lists the possible destinations\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e8c342c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'physics: Good for answering questions about physics\\nmath: Good for answering math questions\\nHistory: Good for answering history questions\\ncomputer science: Good for answering computer science questions'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "destinations_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a643ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the router prompt template\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of “destination” MUST match one of \\\n",
    "the candidate prompts listed below.\\\n",
    "If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4bb97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleja\\AppData\\Local\\Temp\\ipykernel_30908\\3199927026.py:15: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
      "  chain = MultiPromptChain(router_chain=router_chain,\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "# Create the router prompt\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "# Create the router chain and the overall multi-prompt chain\n",
    "router_chain = LLMRouterChain.from_llm(chat_model, router_prompt)\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a209e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': \"What is black body radiation and how does it relate to Planck's law?\"}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Black body radiation refers to the electromagnetic radiation emitted by an object due solely to its temperature, without any specific absorption or emission characteristics. In other words, if an object absorbs all incident electromagnetic radiation regardless of frequency or polarization, then it is a perfect black body.\\n\\nPlanck's law describes the distribution of energy radiated by a black body in thermal equilibrium at a given temperature. The key feature of Planck's law is that it shows how the intensity of emitted radiation depends on both wavelength and temperature. Specifically, the law states that the spectral radiance (the amount of power per unit area per unit solid angle per unit frequency) of a black body increases with increasing frequency up to a peak value, after which it decreases.\\n\\nPlanck's law is given by:\\n\\n\\\\[ B(\\\\lambda, T) = \\\\frac{2hc^2}{\\\\lambda^5} \\\\cdot \\\\frac{1}{e^{h\\\\nu / kT} - 1} \\\\]\\n\\nwhere:\\n- \\\\(B(\\\\lambda, T)\\\\) is the spectral radiance,\\n- \\\\(h\\\\) is Planck's constant (\\\\(6.626 \\\\times 10^{-34}\\\\) Js),\\n- \\\\(c\\\\) is the speed of light in a vacuum (\\\\(3.00 \\\\times 10^8\\\\) m/s),\\n- \\\\(\\\\lambda\\\\) is the wavelength,\\n- \\\\(\\\\nu = \\\\frac{hc}{\\\\lambda}\\\\) is the frequency,\\n- \\\\(k\\\\) is Boltzmann's constant (\\\\(1.380649 × 10^{-23} J/K\\\\)),\\n- \\\\(T\\\\) is the absolute temperature.\\n\\nThe term \\\\(e^{h\\\\nu / kT} - 1\\\\) in Planck's law accounts for the quantization of energy, which means that only certain discrete amounts (quanta) of energy are allowed. This led to Max Planck proposing his quantum hypothesis, which was a significant step towards understanding black body radiation and paved the way for quantum mechanics.\\n\\nIn summary, black body radiation is about how objects emit electromagnetic radiation based on their temperature, while Planck's law describes precisely how this emission varies with wavelength and temperature. The relationship between them is that Planck's law quantitatively expresses the distribution of energy radiated by a black body in thermal equilibrium, incorporating the concept of quantum energy levels.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac5a18",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
