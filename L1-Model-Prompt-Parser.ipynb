{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b08580c",
   "metadata": {},
   "source": [
    "# Load the LLM\n",
    "\n",
    "The first step would be to create a method able to receive a prompt a generate a response given the model name. Please, make sure you have downloaded the model before using `ollama pull`.\n",
    "\n",
    "**Note**: The first time you use this method, it will take some seconds to process a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9da347f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: The capital of France is Paris.\n",
      "response: I am really frustrated because my blender lid popped off and splattered the smoothie all over my kitchen walls! And it gets worse - the warranty doesn't cover the cost of cleaning up the mess. I need your help right now, matey!\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "def get_completion(prompt:str, model_name:str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a response from the LLM model given a prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input prompt to the model.\n",
    "        model_name (str): The name of the model to use.\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the model.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# Example usage\n",
    "response = get_completion(\"What is the capital of France?\", \"qwen2.5:3b\")\n",
    "print(f\"response: {response}\")\n",
    "# Example usage - Create a more complex prompt\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone.    \n",
    "\"\"\"\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "prompt = f\"\"\"\n",
    "Translate the text \\\n",
    "taht is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{customer_email}```. Please, remove any backticks in your response.\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt, \"qwen2.5:3b\")\n",
    "print(f\"response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316e09c",
   "metadata": {},
   "source": [
    "# Use of LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69023cbf",
   "metadata": {},
   "source": [
    "Now, we are going to try something similar, but using LangChain. The first step is to create the chat model. To do so, we have to use the object `ChatOllama`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f46449eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama(model='qwen2.5:3b', temperature=0.0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "chat = ChatOllama(model=\"qwen2.5:3b\", temperature=0.0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919f53ed",
   "metadata": {},
   "source": [
    "Then, we create a template string. As we can see, it's not going to be a formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a994a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "    that is delimited by triple backticks \\\n",
    "    into a style that is {style}. \\\n",
    "    text: ```{text}```\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d75e078",
   "metadata": {},
   "source": [
    "The next step is to create the prompt template object from langchain, using the previous prompt. This is done using `ChatPromptTemplate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad9bbbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], input_types={}, partial_variables={}, template='Translate the text     that is delimited by triple backticks     into a style that is {style}.     text: ```{text}```')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd3357c",
   "metadata": {},
   "source": [
    "We can observe that the prompt object has two **input variables: style and text**. We are going to define these variables and create the final input message for the chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "853a7067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "custom_messages = prompt_template.format_messages(style=customer_style, text=customer_email)\n",
    "print(type(custom_messages[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaf5db0",
   "metadata": {},
   "source": [
    "Finally, we call the model using the custom message we have created before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c1c9641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```\\nI am quite frustrated because my blender lid popped off and sprayed my kitchen walls with smoothie all over them. And unfortunately, the warranty does not cover the expense of cleaning up the mess in my kitchen. I need your assistance urgently, fellow pirate.\\n```' additional_kwargs={} response_metadata={'model': 'qwen2.5:3b', 'created_at': '2025-10-18T14:43:36.1918589Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1214117800, 'load_duration': 23594500, 'prompt_eval_count': 113, 'prompt_eval_duration': 36000000, 'eval_count': 54, 'eval_duration': 1150000000} id='run--8536cbed-697e-4223-a96e-5e004762007e-0'\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "response = chat(custom_messages)\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2c7a0",
   "metadata": {},
   "source": [
    "We can observe that the returned variable by the LLM is not a plain text. It corresponds to an `AIMessage`, a type of message in LangChain, usually generated by a LLM model. In order to convert these type of messages to desired format, we need an output parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "676ef433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='```json\\n{\\n  \"gift\": True,\\n  \"delivery_days\": 2,\\n  \"price_value\": [\"It\\'s slightly more expensive than the other leaf blowers out there, but I think it\\'s worth it for the extra features.\"]\\n}\\n```' additional_kwargs={} response_metadata={'model': 'qwen2.5:3b', 'created_at': '2025-10-18T14:43:39.6153551Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 1367453400, 'load_duration': 47759700, 'prompt_eval_count': 240, 'prompt_eval_duration': 137000000, 'eval_count': 53, 'eval_duration': 1179000000} id='run--9f1c2ed1-77a5-4b85-8fba-0ce63a0be529-0'\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n"
     ]
    }
   ],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\"\n",
    "\n",
    "# Create the prompt template object from the review template\n",
    "prompt_template = ChatPromptTemplate.from_template(review_template)\n",
    "# Create the messages for the chat model\n",
    "messages = prompt_template.format_messages(text=customer_review)\n",
    "# Call the chat model with the messages\n",
    "response = chat(messages)\n",
    "print(response)\n",
    "print(type(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161a8645",
   "metadata": {},
   "source": [
    "As we can see, the model returned a JSON format response, since we required it in the prompt. We are going to parse this output into a Python dictionary, since it's the required format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9707dba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"gift\": string  // Was the item purchased                             as a gift for someone else?                              Answer True if yes,                             False if not or unknown.\n",
      "\t\"delivery_days\": string  // How many days                                      did it take for the product                                      to arrive? If this                                       information is not found,                                      output -1.\n",
      "\t\"price_value\": string  // Extract any                                    sentences about the value or                                     price, and output them as a                                     comma separated Python list.\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "# We create a schema for each of the expected outputs keys\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased\\\n",
    "                             as a gift for someone else? \\\n",
    "                             Answer True if yes,\\\n",
    "                             False if not or unknown.\")\n",
    "delivery_days_schema = ResponseSchema(name=\"delivery_days\",\n",
    "                                      description=\"How many days\\\n",
    "                                      did it take for the product\\\n",
    "                                      to arrive? If this \\\n",
    "                                      information is not found,\\\n",
    "                                      output -1.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any\\\n",
    "                                    sentences about the value or \\\n",
    "                                    price, and output them as a \\\n",
    "                                    comma separated Python list.\")\n",
    "\n",
    "response_schemas = [gift_schema, \n",
    "                    delivery_days_schema,\n",
    "                    price_value_schema]\n",
    "\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print(format_instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe93bd80",
   "metadata": {},
   "source": [
    "The variable `format_conclusions` is a string with orders about the output format of the model must be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9611bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': 'True', 'delivery_days': '2', 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n"
     ]
    }
   ],
   "source": [
    "review_template_2 = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product\\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template_2)\n",
    "\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instructions)\n",
    "\n",
    "response = chat(messages)\n",
    "output_dict = output_parser.parse(response.content)\n",
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3546ee0",
   "metadata": {},
   "source": [
    "As we can see, we got a dict type response, instead of AIMessage object with plain text in it's `content` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e7d048",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
